---
title: "p8105_hw3_zv2138"
output: github_document
---

```{r Setup, echo = FALSE}
library(tidyverse)
library(p8105.datasets)
library(lubridate)
library(kimisc)
```

# Problem 1 
**Cleaning Instacart Data**
```{r Clean Data, message = FALSE}
data("instacart")
instacart = 
  instacart %>% 
    mutate(
      order_dow = order_dow + 1,
      order_dow = wday(order_dow, label = T)
   ) 
```

**Describing the Instacart Dataset** 
The Instacart dataset has `r nrow(instacart)` observations with orders coming from `r length(unique(pull(instacart, order_id)))` costumers. There are `r length(unique(pull(instacart, product_name)))` products displayed in this dataset. Some examples of these products include `r head(unique(pull(instacart, product_name)),3)`. 

The dataset has `r ncol(instacart)` variables including the hour of the day an order was placed, the day of the week an order was placed, and the products in the order with the aisle the product came form.

The dataset includes `r length(unique(pull(instacart, department)))` grocery store departments. Some examples of these departments include `r head(unique(pull(instacart, department)), 3)`. 

## Problem 1 Illustrative Examples 
```{r, message = FALSE}
instacart %>% 
  select(order_id, order_dow, order_hour_of_day) %>% 
  distinct() %>% 
  group_by(order_dow) %>% 
  ggplot(aes(x = order_dow, y = order_hour_of_day)) +
   geom_boxplot(aes(fill = order_dow), alpha = .5) +
   labs(
    title = "Hour of the Day when Orders are Placed ",
    x = "Order Day of the Week",
    y = "Order Hour of the Day",
  ) + 
    theme(legend.position = "none")
```
In general most orders are placed Sunday through Saturday between hours 10 to 17. On Sunday there are several outliers of orders that ordered before 5am. On Saturday the median hour of the day when orders were places is 13.

```{r, message = FALSE}
instacart_dept_dow = instacart %>% 
  filter(department == "alcohol" | department == "pets") %>% 
  group_by(order_dow, aisle, department) %>% 
  summarise(total = n())

instacart_dept_dow %>% 
  ggplot(aes(x = order_dow, y = total, group = aisle, color = aisle)) + 
  geom_line() +
  facet_grid(department ~ ., scales = "free_y") +
  theme(legend.position = "bottom") +
   labs(
    title = "Quantity of Products over the Week taken from Aisles in the Department",
    x = "Day of the Week",
    y = "Quantity of Products",
  )
```
We can see that the most beer coolers are ordered on Thursday and Friday. Pet food is ordered the most on the weekends. 

## Problem 1 Questions 
How many aisles are there, and which aisles are the most items ordered from?
```{r, message = FALSE}
aisles = instacart %>% 
 count(aisle, sort = TRUE)
```
There are `r nrow(aisles)`. The top 5 aisles where most items are ordered are from `r head(pull(aisles, aisle), 5)`

Here we see a plot that shows the number of items ordered in each aisle with more than 10,000 items ordered.
```{r, message = FALSE}
aisles %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = reorder(aisle, n), y = n)) + 
  geom_bar(stat ='identity') + coord_flip() +
  ylab("Total Items Ordered") + xlab("Aisle Name") + 
  ggtitle("Total Items Ordered from Each Aisle with 
              more than 10,000 items Ordered") 
```
The aisle with the most items ordered are `fresh vegetables`, and the aisle with the least items ordered is `butter`.


Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r, message = FALSE}
instacart %>% 
  filter(aisle %in% c('baking ingredients', 'dog food care','packaged vegetables fruits')) %>% 
  count(product_name, aisle, sort = TRUE) %>% 
  group_by(aisle) %>% slice(1:3) %>% 
  knitr::kable()
```
In the `packaged vegetables fruits` aisle the most popular items ordered are all organic. Interestingly `Organic Baby Spinach` is ordered more than `Organic Blueberries`. In the `dog food care` aisle `Snack Sticks Chicken & Rice Recipe Dog Treats` has been ordered 30 times. Interesting in the `baking ingredients` aisle, `Light Brown Sugar` was ordered almost 100 more times than `Cane Sugar`.


Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r, message = FALSE}
table = instacart %>% 
  filter(product_name %in% c('Pink Lady Apples', 'Coffee Ice Cream')) %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(product_name, order_dow) %>% 
  mutate(mean_time_of_day = seconds.to.hms(mean(order_hour_of_day*60*60))) %>% 
  select(product_name, order_dow, mean_time_of_day) %>% 
  distinct() %>% 
  pivot_wider(product_name, names_from = "order_dow", values_from = "mean_time_of_day") 

table[, c("product_name","Sun","Mon","Tue","Wed","Thu","Fri","Sat")] %>% 
   knitr::kable()
```
The mean hour of the day in which `Coffe Ice Cream` is ordered ranges from 12:15 to 15:22. The mean hour is latest in the day on Tuesday and earliest in the day on Friday.
The mean hour of the day in which `Pink Lady Apples` is ordered ranges from 11:21 to 12:47. The mean hour is latest in the day on Friday and earliest in the day on Monday.
Interestingly `Coffe Ice Cream` is on average ordered 2 hours later than `Pink Lady Apples` on Saturday. 


# Problem 2
**Cleaning BRFSS SMART 2010**
```{r, message = FALSE}
data(brfss_smart2010) 
brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() 

brfss_subset = brfss_smart2010 %>%  
  filter(topic %in% "Overall Health") %>%
  filter(response %in% c("Poor","Fair","Good","Very good","Excellent")) %>%
  mutate(response = factor(response, ordered = TRUE, 
                       levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  mutate(locationdesc = gsub("^.{0,5}", "", locationdesc)) %>% 
  rename(state = locationabbr)
```

## Problem 2 Questions 

In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r, message = FALSE}
states_02 = brfss_subset %>% 
  filter(year == '2002') %>% 
  select(locationdesc, state) %>% 
  unique() %>% 
  count(state)

states_02 = states_02 %>% 
  filter(n >= 7)

states_10 = brfss_subset %>% 
  filter(year == '2010') %>% 
  select(locationdesc, state) %>% 
  unique() %>% 
  count(state)

states_10 = states_10 %>% 
  filter(n >= 7)
```
In 2002, `r pull(states_02, state)`  states were observed at 7 or more locations. 
In 2010, `r pull(states_10, state)`  states were observed at 7 or more locations. 


```{r,, message = FALSE}
brfss_spaghetti = brfss_subset %>%
  filter(response == 'Excellent') %>% 
  group_by(state, year) %>% 
  mutate(average_data_value = mean(data_value, na.rm = TRUE)) %>% 
  select(year, state, average_data_value) %>% 
  unique()

ggplot(data = brfss_spaghetti, aes(x = year, y = average_data_value, group = state, color = state)) +
  geom_line() + 
  geom_point(size = 0.4, alpha = 0.5) +
  labs(
    title = "Average Data Value Over the Years by State",
    x = "Year",
    y = "Average Data Value",
  ) 
```
Most states have an average data value ranging from 20 to 26 from 2002 to 2010. It appears that over this time frame there is a slight decrease in the average data value. 

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.
```{r, message = FALSE}
brfss_ny = brfss_subset %>% 
  filter(year == "2006" | year == "2010", state == "NY") %>% 
  select(year, locationdesc, response, data_value)

brfss_ny %>% 
  ggplot(aes(x = data_value, fill = response)) + 
  geom_density(alpha = 0.4) + 
  facet_grid(year ~ .) +
  theme(legend.position = "bottom") +
   labs(
    x = "Average Data Value",
    y = "Density"
  )
```
There does not appear to be a huge difference in the distribution of data value for responses among locations in New York State comparing 2006 to 2010. In both years states with an average data value of 0 to 5 had mostly poor responses. Interesting in 2006 there appears to be a peak in Fair responses for states with an average data value around 12. 

# Problem 3
**Load and Tidy the Accelerometer Data**
```{r, message = FALSE}
accel_data_og = read_csv("./accel_data.csv") 

accel_data = read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(activity_1:activity_1440,
               names_to = "minute",
               values_to = "activity_data") %>% 
  mutate(minute = gsub("^.{0,9}", "", minute),
         minute = as.numeric(minute),
         day = factor(day, ordered = TRUE, 
                  levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
         day_type = ifelse((day == c("Sunday", "Saturday")), "Weekend", "Weekday")) %>% 
  relocate(day_type, .after = day)
```
The Accelerometer dataset has `r nrow(accel_data)` observations `r ncol(accel_data)` variables. Variables include `week` which refers to the week number the data was collected. Data was collected over `r length(unique((pull(accel_data, week))))` weeks. The data contains a `minute` variable which represents the minute of the data the data is from. The data also contains an `activity_data` variable indicating the level of activity for a given `minute`. 

Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?
```{r, message = FALSE}
total_activity = accel_data %>% 
  group_by(day_id, day) %>% 
  summarize(total_activity = sum(activity_data)) 

knitr::kable(total_activity)

total_activity %>% 
  group_by(day_id) %>% 
  ggplot(aes(x = day_id, y = total_activity, group = day)) + 
  geom_boxplot(aes(fill = day), alpha = .5) +
   labs(
    x = "Day of the Week",
    y = "Total Activity"
  )
```
The most activity occurred on Friday. Saturday has the most spread in total activity count while Wednesday has the least spread. Thursday and Tuesday have similar median level of total activity. 


Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. 
```{r, message = FALSE}
accel_data %>% 
  ggplot(aes(x = minute, y = activity_data, group = day, color = day)) +
  geom_point(size = 0.4, alpha = 0.5) + 
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), size = 0.4, alpha = 0.5) +
  labs(
    title = "24-Hour Activity for Each Day of the Week",
    x = "Activity Minute",
    y = "Activtiy Value",
  ) 
```
Each dot represents the minute an activity occurred and the value of activity data. Each dot is color coded to indicated the day of the week. We see over the course of a 24 hour day activity value begins to increase at around minute 400. 