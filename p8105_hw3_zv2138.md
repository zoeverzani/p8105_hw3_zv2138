p8105\_hw3\_zv2138
================

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──

    ## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4
    ## ✓ tibble  3.1.2     ✓ dplyr   1.0.7
    ## ✓ tidyr   1.1.3     ✓ stringr 1.4.0
    ## ✓ readr   2.0.1     ✓ forcats 0.5.1

    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

    ## 
    ## Attaching package: 'lubridate'

    ## The following objects are masked from 'package:base':
    ## 
    ##     date, intersect, setdiff, union

# Problem 1

**Cleaning Instacart Data**

``` r
data("instacart")
instacart = 
  instacart %>% 
    mutate(
      order_dow = order_dow + 1,
      order_dow = wday(order_dow, label = T)
   ) 
```

**Describing the Instacart Dataset** The Instacart dataset has 1384617
observations with orders coming from 131209 costumers. There are 39123
products displayed in this dataset. Some examples of these products
include Bulgarian Yogurt, Organic 4% Milk Fat Whole Milk Cottage Cheese,
Organic Celery Hearts.

The dataset has 15 variables including the hour of the day an order was
placed, the day of the week an order was placed, and the products in the
order with the aisle the product came form.

The dataset includes 21 grocery store departments. Some examples of
these departments include dairy eggs, produce, canned goods.

## Problem 1 Illustrative Examples

``` r
instacart %>% 
  select(order_id, order_dow, order_hour_of_day) %>% 
  distinct() %>% 
  group_by(order_dow) %>% 
  ggplot(aes(x = order_dow, y = order_hour_of_day)) +
   geom_boxplot(aes(fill = order_dow), alpha = .5) +
   labs(
    title = "Hour of the Day when Orders are Placed ",
    x = "Order Day of the Week",
    y = "Order Hour of the Day",
  ) + 
    theme(legend.position = "none")
```

![](p8105_hw3_zv2138_files/figure-gfm/unnamed-chunk-1-1.png)<!-- --> In
general most orders are placed Sunday through Saturday between hours 10
to 17. On Sunday there are several outliers of orders that ordered
before 5am. On Saturday the median hour of the day when orders were
places is 13.

``` r
instacart_dept_dow = instacart %>% 
  filter(department == "alcohol" | department == "pets") %>% 
  group_by(order_dow, aisle, department) %>% 
  summarise(total = n())

instacart_dept_dow %>% 
  ggplot(aes(x = order_dow, y = total, group = aisle, color = aisle)) + 
  geom_line() +
  facet_grid(department ~ ., scales = "free_y") +
  theme(legend.position = "bottom") +
   labs(
    title = "Quantity of Products over the Week taken from Aisles in the Department",
    x = "Day of the Week",
    y = "Quantity of Products",
  )
```

![](p8105_hw3_zv2138_files/figure-gfm/unnamed-chunk-2-1.png)<!-- --> We
can see that the most beer coolers are ordered on Thursday and Friday.
Pet food is ordered the most on the weekends.

## Problem 1 Questions

How many aisles are there, and which aisles are the most items ordered
from?

``` r
aisles = instacart %>% 
 count(aisle, sort = TRUE)
```

There are 134. The top 5 aisles where most items are ordered are from
fresh vegetables, fresh fruits, packaged vegetables fruits, yogurt,
packaged cheese

Here we see a plot that shows the number of items ordered in each aisle
with more than 10,000 items ordered.

``` r
aisles %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = reorder(aisle, n), y = n)) + 
  geom_bar(stat ='identity') + coord_flip() +
  ylab("Total Items Ordered") + xlab("Aisle Name") + 
  ggtitle("Total Items Ordered from Each Aisle with 
              more than 10,000 items Ordered") 
```

![](p8105_hw3_zv2138_files/figure-gfm/unnamed-chunk-4-1.png)<!-- --> The
aisle with the most items ordered are `fresh vegetables`, and the aisle
with the least items ordered is `butter`.

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
Include the number of times each item is ordered in your table.

``` r
instacart %>% 
  filter(aisle %in% c('baking ingredients', 'dog food care','packaged vegetables fruits')) %>% 
  count(product_name, aisle, sort = TRUE) %>% 
  group_by(aisle) %>% slice(1:3) %>% 
  knitr::kable()
```

| product\_name                                 | aisle                      |    n |
|:----------------------------------------------|:---------------------------|-----:|
| Light Brown Sugar                             | baking ingredients         |  499 |
| Pure Baking Soda                              | baking ingredients         |  387 |
| Cane Sugar                                    | baking ingredients         |  336 |
| Snack Sticks Chicken & Rice Recipe Dog Treats | dog food care              |   30 |
| Organix Chicken & Brown Rice Recipe           | dog food care              |   28 |
| Small Dog Biscuits                            | dog food care              |   26 |
| Organic Baby Spinach                          | packaged vegetables fruits | 9784 |
| Organic Raspberries                           | packaged vegetables fruits | 5546 |
| Organic Blueberries                           | packaged vegetables fruits | 4966 |

In the `packaged vegetables fruits` aisle the most popular items ordered
are all organic. Interestingly `Organic Baby Spinach` is ordered more
than `Organic Blueberries`. In the `dog food care` aisle
`Snack Sticks Chicken & Rice Recipe Dog Treats` has been ordered 30
times. Interesting in the `baking ingredients` aisle,
`Light Brown Sugar` was ordered almost 100 more times than `Cane Sugar`.

Make a table showing the mean hour of the day at which Pink Lady Apples
and Coffee Ice Cream are ordered on each day of the week; format this
table for human readers (i.e. produce a 2 x 7 table).

``` r
table = instacart %>% 
  filter(product_name %in% c('Pink Lady Apples', 'Coffee Ice Cream')) %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(product_name, order_dow) %>% 
  mutate(mean_time_of_day = seconds.to.hms(mean(order_hour_of_day*60*60))) %>% 
  select(product_name, order_dow, mean_time_of_day) %>% 
  distinct() %>% 
  pivot_wider(product_name, names_from = "order_dow", values_from = "mean_time_of_day") 

table[, c("product_name","Sun","Mon","Tue","Wed","Thu","Fri","Sat")] %>% 
   knitr::kable()
```

| product\_name    | Sun      | Mon      | Tue      | Wed      | Thu      | Fri      | Sat      |
|:-----------------|:---------|:---------|:---------|:---------|:---------|:---------|:---------|
| Pink Lady Apples | 13:26:28 | 11:21:36 | 11:42:07 | 14:15:00 | 11:33:06 | 12:47:03 | 11:56:15 |
| Coffee Ice Cream | 13:46:27 | 14:18:56 | 15:22:51 | 15:19:05 | 15:13:02 | 12:15:47 | 13:50:00 |

The mean hour of the day in which `Coffe Ice Cream` is ordered ranges
from 12:15 to 15:22. The mean hour is latest in the day on Tuesday and
earliest in the day on Friday. The mean hour of the day in which
`Pink Lady Apples` is ordered ranges from 11:21 to 12:47. The mean hour
is latest in the day on Friday and earliest in the day on Monday.
Interestingly `Coffe Ice Cream` is on average ordered 2 hours later than
`Pink Lady Apples` on Saturday.

# Problem 2

**Cleaning BRFSS SMART 2010**

``` r
data(brfss_smart2010) 
brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() 

brfss_subset = brfss_smart2010 %>%  
  filter(topic %in% "Overall Health") %>%
  filter(response %in% c("Poor","Fair","Good","Very good","Excellent")) %>%
  mutate(response = factor(response, ordered = TRUE, 
                       levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  mutate(locationdesc = gsub("^.{0,5}", "", locationdesc)) %>% 
  rename(state = locationabbr)
```

## Problem 2 Questions

In 2002, which states were observed at 7 or more locations? What about
in 2010?

``` r
states_02 = brfss_subset %>% 
  filter(year == '2002') %>% 
  select(locationdesc, state) %>% 
  unique() %>% 
  count(state)

states_02 = states_02 %>% 
  filter(n >= 7)

states_10 = brfss_subset %>% 
  filter(year == '2010') %>% 
  select(locationdesc, state) %>% 
  unique() %>% 
  count(state)

states_10 = states_10 %>% 
  filter(n >= 7)
```

In 2002, CT, FL, MA, NC, NJ, PA states were observed at 7 or more
locations. In 2010, CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX,
WA states were observed at 7 or more locations.

``` r
brfss_spaghetti = brfss_subset %>%
  filter(response == 'Excellent') %>% 
  group_by(state, year) %>% 
  mutate(average_data_value = mean(data_value, na.rm = TRUE)) %>% 
  select(year, state, average_data_value) %>% 
  unique()

ggplot(data = brfss_spaghetti, aes(x = year, y = average_data_value, group = state, color = state)) +
  geom_line() + 
  geom_point(size = 0.4, alpha = 0.5) +
  labs(
    title = "Average Data Value Over the Years by State",
    x = "Year",
    y = "Average Data Value",
  ) 
```

![](p8105_hw3_zv2138_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->
Most states have an average data value ranging from 20 to 26 from 2002
to 2010. It appears that over this time frame there is a slight decrease
in the average data value.

Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State.

``` r
brfss_ny = brfss_subset %>% 
  filter(year == "2006" | year == "2010", state == "NY") %>% 
  select(year, locationdesc, response, data_value)

brfss_ny %>% 
  ggplot(aes(x = data_value, fill = response)) + 
  geom_density(alpha = 0.4) + 
  facet_grid(year ~ .) +
  theme(legend.position = "bottom") +
   labs(
    x = "Average Data Value",
    y = "Density"
  )
```

![](p8105_hw3_zv2138_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->
There does not appear to be a huge difference in the distribution of
data value for responses among locations in New York State comparing
2006 to 2010. In both years states with an average data value of 0 to 5
had mostly poor responses. Interesting in 2006 there appears to be a
peak in Fair responses for states with an average data value around 12.

# Problem 3

**Load and Tidy the Accelerometer Data**

``` r
accel_data_og = read_csv("./accel_data.csv") 

accel_data = read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(activity_1:activity_1440,
               names_to = "minute",
               values_to = "activity_data") %>% 
  mutate(minute = gsub("^.{0,9}", "", minute),
         minute = as.numeric(minute),
         day = factor(day, ordered = TRUE, 
                  levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
         day_type = ifelse((day == c("Sunday", "Saturday")), "Weekend", "Weekday")) %>% 
  relocate(day_type, .after = day)
```

The Accelerometer dataset has 50400 observations 6 variables. Variables
include `week` which refers to the week number the data was collected.
Data was collected over 5 weeks. The data contains a `minute` variable
which represents the minute of the data the data is from. The data also
contains an `activity_data` variable indicating the level of activity
for a given `minute`.

Using your tidied dataset, aggregate accross minutes to create a total
activity variable for each day, and create a table showing these totals.
Are any trends apparent?

``` r
total_activity = accel_data %>% 
  group_by(day_id, day) %>% 
  summarize(total_activity = sum(activity_data)) 

knitr::kable(total_activity)
```

| day\_id | day       | total\_activity |
|--------:|:----------|----------------:|
|       1 | Friday    |       480542.62 |
|       2 | Monday    |        78828.07 |
|       3 | Saturday  |       376254.00 |
|       4 | Sunday    |       631105.00 |
|       5 | Thursday  |       355923.64 |
|       6 | Tuesday   |       307094.24 |
|       7 | Wednesday |       340115.01 |
|       8 | Friday    |       568839.00 |
|       9 | Monday    |       295431.00 |
|      10 | Saturday  |       607175.00 |
|      11 | Sunday    |       422018.00 |
|      12 | Thursday  |       474048.00 |
|      13 | Tuesday   |       423245.00 |
|      14 | Wednesday |       440962.00 |
|      15 | Friday    |       467420.00 |
|      16 | Monday    |       685910.00 |
|      17 | Saturday  |       382928.00 |
|      18 | Sunday    |       467052.00 |
|      19 | Thursday  |       371230.00 |
|      20 | Tuesday   |       381507.00 |
|      21 | Wednesday |       468869.00 |
|      22 | Friday    |       154049.00 |
|      23 | Monday    |       409450.00 |
|      24 | Saturday  |         1440.00 |
|      25 | Sunday    |       260617.00 |
|      26 | Thursday  |       340291.00 |
|      27 | Tuesday   |       319568.00 |
|      28 | Wednesday |       434460.00 |
|      29 | Friday    |       620860.00 |
|      30 | Monday    |       389080.00 |
|      31 | Saturday  |         1440.00 |
|      32 | Sunday    |       138421.00 |
|      33 | Thursday  |       549658.00 |
|      34 | Tuesday   |       367824.00 |
|      35 | Wednesday |       445366.00 |

``` r
total_activity %>% 
  group_by(day_id) %>% 
  ggplot(aes(x = day_id, y = total_activity, group = day)) + 
  geom_boxplot(aes(fill = day), alpha = .5) +
   labs(
    x = "Day of the Week",
    y = "Total Activity"
  )
```

![](p8105_hw3_zv2138_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->
The most activity occurred on Friday. Saturday has the most spread in
total activity count while Wednesday has the least spread. Thursday and
Tuesday have similar median level of total activity.

Make a single-panel plot that shows the 24-hour activity time courses
for each day and use color to indicate day of the week.

``` r
accel_data %>% 
  ggplot(aes(x = minute, y = activity_data, group = day, color = day)) +
  geom_point(size = 0.4, alpha = 0.5) + 
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), size = 0.4, alpha = 0.5) +
  labs(
    title = "24-Hour Activity for Each Day of the Week",
    x = "Activity Minute",
    y = "Activtiy Value",
  ) 
```

![](p8105_hw3_zv2138_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->
Each dot represents the minute an activity occurred and the value of
activity data. Each dot is color coded to indicated the day of the week.
We see over the course of a 24 hour day activity value begins to
increase at around minute 400.
